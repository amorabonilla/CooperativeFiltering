---
title: "Experiment with CelebFaces"
author: "P. Cordero, M. Enciso, A. Mora, C. Rossi"
date: "11/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
The CelebFaces Attributes (CelebA) Dataset data set  (over 200k images of celebrities with 40 binary attribute annotations).   This dataset is in [https://www.kaggle.com/jessicali9530/celeba-dataset](https://www.kaggle.com/jessicali9530/celeba-dataset)  and it is very used  for training and testing models for face detection, particularly for recognising facial attributes such as finding people with brown hair, are smiling, or wearing glasses.  The dimensions are 202599 rows (persons) and 40 columns (attributes, characteristics of a person).



Firstly, a trace of the execution is showed and at the end of the document, it appears a summary of the results. 

## Importing the dataset

```{r, echo=FALSE, warning=FALSE, message=FALSE, display=FALSE}
library(devtools)
devtools::load_all()
library(knitr)
library(arules)
library(fcaR)
library(Matrix)


library(tidyverse)
library(pmml)
library(sets)
library(magrittr)
# library("stringr")
# library(data.table)
# library(readxl)
# #library(readr)

library(foreach)
library(doParallel)
library(doSNOW)
# library(foreach)

#read_chunk("R/main.R")


# setwd("/Volumes/GoogleDrive/Mi unidad/CooperativeFiltering")

FC <- read_csv("datasets/list_attr_celeba.csv")
indicex <- FC==-1
FC[indicex] <- 0

colnames(FC)
filas <- rownames(FC)
FC1 <-as.data.frame(sapply(FC[,2:41], function(x) as.logical(x)))
#View(head(FC1))

```

The size of the dataset is: 



```{r}
dim(FC)
```

## Extracting implications

We extract the rules:
```{r, echo=FALSE, warning=FALSE, message=FALSE, display=FALSE}

my_support <- 0.001
rule_param = list(
  supp = my_support,
  conf = 1,
  maxlen = 15
)
dt_experiment_sizes <- data.frame(execution=c(),
                                  experiment=c(),
                                  iteration=c(),
                                  sigma=c(),
                                  #closure=c(),
                                  attributes=c(),
                                  objects=c(),
                                  closure=c())







#fc_movielens$implications
#fc_movielens$implications$apply_rules(rules = c("composition",
#                                                "simplification"))

#Rules <- fc_movielens$implications$to_arules(quality = FALSE)

#write.PMML(Rules, file = name_file_rules)

num_execution <- 1
num_experiment <- 1
verbose <- FALSE

# n number of users
n_users=2
num_scales <- 4

number_executions_of_the_experiment <- 1000

Rules <- apriori(FC1,parameter = rule_param)
name_file_rules <- "reglas_celebA.xml"



Rules1 <- Rules[!is.redundant(Rules)]



for (k in 1:number_executions_of_the_experiment){

Rules <- Rules1

#  write.PMML(Rules, file = name_file_rules)



  recommendation <- random_experiment_dataset(Rules,FC1,name_file_rules,
                                              n_users,num_scales,
                                              my_support,
                                              num_execution,
                                              num_experiment, verbose )

  dt_experiment_sizes <- rbind(dt_experiment_sizes,recommendation$pref_sizes )
  num_experiment <- num_experiment +1

}
dt_experiment_sizes$execution <- NULL
 
dt_experiment_sizes
write_csv(dt_experiment_sizes,"celeb_sizes.csv")
```
 
 
The maximun number of diseases in the dataset is:
```{r}
max_objects <-dt_experiment_sizes$objects[1]
max_objects
```

## Results

After the execution of our method, we show a summary of the number of diseases identified in each step of the execution of our recommender system:

```{r, echo=FALSE}
summary_by_iteration <- dt_experiment_sizes %>%
  rename(step =iteration) %>%
  group_by(step) %>%
  summarise(
    n = n(),
    max_objects = max(objects, na.rm = TRUE),
  )  %>%
  mutate(prunning_objects = 100-(max_objects/.GlobalEnv$max_objects)*100)

summary_by_iteration
```
 
 
 
